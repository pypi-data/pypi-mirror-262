"""Adjust model decision thresholds."""

import numpy as np
from scipy import optimize
from typing import List, Union, Optional, Callable
import logging
from logging import NullHandler

from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.metrics import matthews_corrcoef
from extendanything import ExtendAnything

__author__ = """Maxim Zaslavsky"""
__email__ = "maxim@maximz.com"
__version__ = "0.0.1"

# Set default logging handler to avoid "No handler found" warnings.
logging.getLogger(__name__).addHandler(NullHandler())


class AdjustedProbabilitiesDerivedModel(ExtendAnything, ClassifierMixin, BaseEstimator):
    """Wrapper around any multiclass classification model to adjust decision thresholds that determine predicted labels.
    The output of predict() and predict_proba() are different:
    - predict_proba() probabilities are rescaled and may no longer sum to 1.
    - predict() is the argmax of the rescaled predict_proba() probabilities.

    Applying the class reweighting operation to the predicted probabilities does not affect the final ROC AUC score:
    the ROC AUC stays the same after multiplying the class probabilities by their weights.

    But the ROC AUC can change if you then renormalize the probabilities to sum to 1 again,
    because the rank orderings between examples may change.
    Don't renormalize.

    Note: sklearn's built-in ROC AUC score does not accept multiclass probabilities that don't sum to 1.
    Use crosseval roc_auc_score instead.

    The intuition for how this adjusts decision thresholds between classes:
    By downweighing a class by dividing its probabilities by a factor of n, it now has to be n times as strong to outrank its competitor classes.
    For each example: Apply these class-specific factors, then choose dominant class probability as the winning label.

    Based on:
    - https://stats.stackexchange.com/a/310956/297
    - https://stats.stackexchange.com/a/449903/297
    """

    # TODO: Can we make AdjustedProbabilitiesDerivedModel be a generic type that accepts a type T and shadows all Tâ€™s type params?

    # ExtendAnything means will pass through to base instance's attributes.
    # Used to map e.g. classes_ -> inner_clf.classes_ and predict_proba -> inner_clf.predict_proba.

    # TODO: is there a type hint for all sklearn models?
    # BaseEstimator is odd to put here: https://stackoverflow.com/questions/54868698/what-type-is-a-sklearn-model
    # Preferably something that requires predict_proba() to exist.
    def __init__(
        self, inner_clf: BaseEstimator, class_weights: Union[np.ndarray, List[float]]
    ) -> None:
        # sets self._inner
        super().__init__(inner_clf)
        # also make it available as self.inner_clf so that sklearn clone works
        self.inner_clf = inner_clf

        self.class_weights = class_weights

    @staticmethod
    def _compute_adjusted_class_probabilities(
        predict_probas: np.ndarray,
        class_weights: Union[np.ndarray, List[float]],
    ) -> np.ndarray:
        # Element-wise multiply - i.e. reweight the classes
        # Don't renormalize each row to sum to 1.
        return np.multiply(predict_probas, class_weights)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        Reweight predicted class probability vectors.
        """
        return self._compute_adjusted_class_probabilities(
            self._inner.predict_proba(X), self.class_weights
        )

    @staticmethod
    def _choose_winning_label(
        classes: Union[List[str], List[int], np.ndarray],
        adjusted_probabilities: np.ndarray,
    ) -> Union[str, int, np.ndarray]:
        # Argmax along last axis,
        # which is axis 1 for a 2D array and axis 0 for a 1D array.

        # This handles edge case when adjusted_probailities is a 1d array, i.e. X had only one example.
        # Don't want to convert to 2d array because in this case.
        # Return a scalar (this is the str in the union type), rather than an array with one entry.

        # Defensive casting:
        return np.array(classes)[np.argmax(np.array(adjusted_probabilities), axis=-1)]

    def predict(self, X: np.ndarray) -> Union[str, int, np.ndarray]:
        """Reweight predicted class probability vectors to choose predicted class label for each example."""
        return self._choose_winning_label(self.classes_, self.predict_proba(X))

    @staticmethod
    def _get_weights_to_optimize_decision_threshold(
        predicted_probabilities_validation: np.ndarray,
        classes: Union[List[str], List[int], np.ndarray],
        y_validation_true: np.ndarray,
        score_func: Callable = matthews_corrcoef,
    ) -> np.ndarray:
        """
        Given predict_proba(X) and y_true, find class weights to maximize accuracy (or another score) of choosing predicted labels y_pred.

        Usage: train a model on a training set, tune its decision thresholds on a validation set with this method, then apply to a test set.

        score_func must accept these parameters: score_func(y_true, y_pred)
        """

        def _score(class_weights, probabilities, y_true, classes):
            # We could follow the standard usage of AdjustedProbabilitiesDerivedModel as follows:
            # adjusted_model = AdjustedProbabilitiesDerivedModel(model, class_weights)
            # y_pred = adjusted_model.predict(X)

            # But for efficiency we only run predict_proba() once and store the result, which will not change as we modify class weights.
            # That's provided in `probabilities` argument.

            y_pred = AdjustedProbabilitiesDerivedModel._choose_winning_label(
                classes,
                AdjustedProbabilitiesDerivedModel._compute_adjusted_class_probabilities(
                    probabilities, class_weights
                ),
            )

            # Compute and return negative since we are minimizing.
            score = score_func(y_true, y_pred)
            return -1.0 * score

        # Check for any classes that were in the model but are now absent from the tuning data.
        # Their weights will be fixed at 0.
        n_classes = len(classes)
        present_classes = np.unique(y_validation_true)
        n_present_classes = len(present_classes)

        # Initialize weights to 1/n_present_classes for classes present in validation, 0 otherwise
        initial_weights = np.array(
            [
                1.0 / n_present_classes if classes[i] in present_classes else 0.0
                for i in range(n_classes)
            ]
        )

        # Define bounds for optimization.
        # Technically, the only constraint we need is >0, so that adjusted probabilities are >0 and normalizing rows by sum works right.
        # But to constrain the search, we'll use the range 0 to 1. (Consider a 1-100 range instead?)
        # Also, we will constrain missing classes to always have 0 weight.
        lower_bounds = [
            1e-5 if classes[i] in present_classes else 0.0 for i in range(n_classes)
        ]
        upper_bounds = [
            1.0 if classes[i] in present_classes else 0.0 for i in range(n_classes)
        ]
        bounds = optimize.Bounds(lower_bounds, upper_bounds)

        # Class weights will be supplied as first argument. args are additional arguments to _score.
        result = optimize.differential_evolution(
            _score,
            bounds,
            args=(
                np.array(predicted_probabilities_validation),
                np.array(y_validation_true),
                classes,
            ),
            # Starting point for the optimization
            x0=initial_weights,
        )
        best_class_weights = result.x
        return best_class_weights

    @classmethod
    def adjust_model_decision_thresholds(
        cls,
        model: BaseEstimator,
        y_validation_true: np.ndarray,
        score_func=matthews_corrcoef,
        X_validation: Optional[np.ndarray] = None,
        predicted_probabilities_validation: Optional[np.ndarray] = None,
    ) -> "AdjustedProbabilitiesDerivedModel":
        """
        Tune a multiclass model's decision thresholds against a validation set:
        Tune class weights to achieve higher classification performance with this set of predicted class probabilities

        Provide X_validation or predicted_probabilities_validation.
        """
        if X_validation is None and predicted_probabilities_validation is None:
            raise ValueError(
                "Must provide X_validation or predicted_probabilities_validation."
            )

        if predicted_probabilities_validation is None:
            # Get predict_probas once, then optimize on them; don't call predict_proba over and over again.
            predicted_probabilities_validation = model.predict_proba(X_validation)
            # Optionally reorder if we have Pandas objects
            if hasattr(y_validation_true, "index"):
                if hasattr(predicted_probabilities_validation, "index"):
                    # This takes precedence
                    y_validation_true = y_validation_true.loc[
                        predicted_probabilities_validation.index
                    ]
                elif hasattr(X_validation, "index"):
                    y_validation_true = y_validation_true.loc[X_validation.index]

        best_class_weights = cls._get_weights_to_optimize_decision_threshold(
            predicted_probabilities_validation=predicted_probabilities_validation,
            classes=model.classes_,
            y_validation_true=y_validation_true,
            score_func=score_func,
        )
        # Return new model (wrapper)
        return cls(model, best_class_weights)
