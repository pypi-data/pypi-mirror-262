{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6084335a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy<3.8.0,>=3.7.2 (from en-core-web-sm==3.7.1)\n",
      "  Using cached spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.9)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.13)\n",
      "Requirement already satisfied: jinja2 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/hobs/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/hobs/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hobs/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hobs/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.17)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2020.6.20)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/hobs/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/hobs/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/hobs/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in /home/hobs/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hobs/.local/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Using cached spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "Installing collected packages: spacy, en-core-web-sm\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.7.1\n",
      "    Uninstalling spacy-3.7.1:\n",
      "      Successfully uninstalled spacy-3.7.1\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.7.0\n",
      "    Uninstalling en-core-web-sm-3.7.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "nudger 0.0.8 requires pandas<2.0.0,>=1.5.3, but you have pandas 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed en-core-web-sm-3.7.1 spacy-3.7.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hobs/.local/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.7.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7eff3b73c780>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "# Smaller BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3eb4f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from CSV...\n",
      "                    filename  \\\n",
      "0  Gum_And_Tooth_Disease.txt   \n",
      "1  Gum_And_Tooth_Disease.txt   \n",
      "2  Gum_And_Tooth_Disease.txt   \n",
      "3  Gum_And_Tooth_Disease.txt   \n",
      "4  Gum_And_Tooth_Disease.txt   \n",
      "\n",
      "                                            sentence  line_number  \n",
      "0  I am thinking of going back on your [Primal] d...            1  \n",
      "1  However could you let me know what are the\\nef...            2  \n",
      "2  The reason I ask is\\nbecause I have gum diseas...            3  \n",
      "3  The strapline to his video was \"a raw meat die...            6  \n",
      "4  He cited\\nthat all of the people he had met on...            6  \n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path.cwd() / \"data\"\n",
    "\n",
    "DF_DIR = Path.cwd() / \"saved_dfs\"\n",
    "DF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df_path = DF_DIR / \"dataframe.csv\"\n",
    "\n",
    "# Conditional that checks whether we saved the dfs as csv files in prior run.\n",
    "# If yes, then reinitialise these csvs as dfs.\n",
    "# If not, then create the dfs and save them in csv format for next run.\n",
    "if df_path.exists():\n",
    "    print(\"Loading dataset from CSV...\")\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "else:\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "\n",
    "            file_path = DATA_DIR / filename\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "\n",
    "            # Create a mapping of character positions to line numbers\n",
    "            line_starts = {0: 1}\n",
    "            for i, char in enumerate(content):\n",
    "                if char == '\\n':\n",
    "                    line_starts[i + 1] = line_starts[i] + 1\n",
    "                else:\n",
    "                    line_starts[i + 1] = line_starts[i]\n",
    "\n",
    "            # Process the entire content with spaCy\n",
    "            doc = nlp(content)\n",
    "            for sent in doc.sents:\n",
    "                start_char = sent.start_char\n",
    "                line_number = line_starts[start_char]\n",
    "                sentence = sent.text.strip()\n",
    "                data.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"sentence\": sentence,\n",
    "                    \"line_number\": line_number\n",
    "                })\n",
    "\n",
    "\n",
    "    # Convert the list of sentence/filename dictionaries into a dataframe \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(df_path, index=False)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f929233f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from file...\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "EMBEDDING_DIR = Path.cwd() / \"embeddings\"\n",
    "EMBEDDING_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "embeddings_path = EMBEDDING_DIR / f'sentence_embeddings.joblib'\n",
    "\n",
    "# Conditional to check whether our embeddings joblib already exists from prior runs.\n",
    "if not embeddings_path.exists():\n",
    "    print(\"Generating embeddings for the dataset...\")\n",
    "    embeddings = model.encode(df['sentence'].tolist(), show_progress_bar=True)\n",
    "    joblib.dump(embeddings, embeddings_path)\n",
    "else:\n",
    "    print(\"Loading embeddings from file...\")\n",
    "    embeddings = joblib.load(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd05dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def search(query, embeddings, df):\n",
    "    # Encode the query string using the model to get its embedding.\n",
    "    query_embedding = model.encode([query]) \n",
    "\n",
    "    # Calculate the cosine similarity between the query embedding and all embeddings in the dataset.\n",
    "    # cosine_similarity returns a matrix where each row is the similarity of the query to each document.\n",
    "    # We take the first row [0] because there's only one query, resulting in a one-dimensional array of similarities.\n",
    "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "\n",
    "    top_indices = np.argsort(similarities)[-20:]\n",
    "\n",
    "    # Use DataFrame.iloc to select the rows at the given indices (top_indices).\n",
    "    # This gives us the rows from the dataframe that correspond to the top 20 similarities.\n",
    "    top_docs = df.iloc[top_indices]\n",
    "\n",
    "    # Select the corresponding top similarity scores using the indices.\n",
    "    # This gives us the actual similarity scores of the top 20 matches.\n",
    "    top_scores = similarities[top_indices]\n",
    "\n",
    "    # Return the top matching documents and their similarity scores.\n",
    "    return top_docs[['sentence', 'filename']], top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95e0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_columns=[\"Query\", \"Result\", \"Cosine\", \"Filename\"]\n",
    "\n",
    "def create_test_set(query, embeddings, df, test_set):\n",
    "    top_docs, top_scores = search(query, embeddings, df)\n",
    "    \n",
    "    if top_docs.empty:\n",
    "        print(\"No documents found for this query.\")\n",
    "        return test_set\n",
    "\n",
    "    new_rows = []\n",
    "    # Use zip to positionally combine and iterate over the df and scores in parallel. \n",
    "    # iterrows() is used to return a tuple of (index, Series) from the df.\n",
    "    for (index, row), score in zip(top_docs.iterrows(), top_scores):\n",
    "        new_row = {\n",
    "            \"Query\": query,\n",
    "            \"Result\": row['sentence'],\n",
    "            \"Cosine\": score,\n",
    "            \"Filename\": row['filename'],\n",
    "        }\n",
    "        new_rows.append(new_row)\n",
    "    \n",
    "    new_rows_df = pd.DataFrame(new_rows)\n",
    "    test_set = pd.concat([test_set, new_rows_df], ignore_index=True)\n",
    "    \n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61276110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20482/2439678623.py:23: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_set = pd.concat([test_set, new_rows_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def generate_test_set_from_queries(query_file_path, embeddings, df):\n",
    "    test_set = pd.DataFrame(columns=test_set_columns)\n",
    "    with open(query_file_path, 'r') as file:\n",
    "        queries = file.read().splitlines()\n",
    "    \n",
    "    for query in queries:\n",
    "        test_set = create_test_set(query, embeddings, df, test_set)\n",
    "    \n",
    "    return test_set\n",
    "\n",
    "TEST_SET_DIR = Path.cwd() / \"test_sets\"\n",
    "QUERIES_DIR = Path.cwd() / \"queries\"\n",
    "TEST_SET_DIR.mkdir(exist_ok=True)\n",
    "QUERIES_DIR.mkdir(exist_ok=True)\n",
    "test_set_path = TEST_SET_DIR / f\"test_set.csv\"\n",
    "query_file_path = QUERIES_DIR / \"queries.txt\"\n",
    "\n",
    "test_set = generate_test_set_from_queries(query_file_path, embeddings, df)\n",
    "\n",
    "test_set.to_csv(test_set_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81ca63d6-9c72-4ea2-9a6a-4352c80168d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                 Query                                             Result  \\\n",
       " 0    Is salt unhealthy  “The craving for salt is symptomatic of a mine...   \n",
       " 1    Is salt unhealthy       When people drink salt water, they get sick.   \n",
       " 2    Is salt unhealthy  So lots of salt, which is cheap and flavorful ...   \n",
       " 3    Is salt unhealthy  I may not have to write this but, all the abov...   \n",
       " 4    Is salt unhealthy  But still you're destroying a lot of cells eve...   \n",
       " ..                 ...                                                ...   \n",
       " 175  Why eat high meat  I've got people who only do high meat, only do...   \n",
       " 176  Why eat high meat  Eating high raw meat supplies the body with na...   \n",
       " 177  Why eat high meat  can be consistently alleviated by eating HIGH ...   \n",
       " 178  Why eat high meat  The bacteria-infested meat, called “high meat”...   \n",
       " 179  Why eat high meat  To facilitate the removal of\\ndegenerative tis...   \n",
       " \n",
       "        Cosine                                    Filename  \n",
       " 0    0.618432                         We_Want_To_Live.txt  \n",
       " 1    0.620940                         We_Want_To_Live.txt  \n",
       " 2    0.627519  Are_Raw_Miso_And_Shoyu_Healthy_Sauces?.txt  \n",
       " 3    0.628249                    Benefits_of_Raw_Eggs.txt  \n",
       " 4    0.629511           Primal_Diet_Workshop_(Part_3).txt  \n",
       " ..        ...                                         ...  \n",
       " 175  0.641372                    Benefits_of_Raw_Eggs.txt  \n",
       " 176  0.645114   The_Recipe_for_Living_Without_Disease.txt  \n",
       " 177  0.668449   The_Recipe_for_Living_Without_Disease.txt  \n",
       " 178  0.719470   The_Recipe_for_Living_Without_Disease.txt  \n",
       " 179  0.740459   The_Recipe_for_Living_Without_Disease.txt  \n",
       " \n",
       " [180 rows x 4 columns],\n",
       " PosixPath('/home/hobs/code/tangibleai/community/vector-search/test_sets'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set, TEST_SET_DIR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae5b34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Query Group  Total Hits  \\\n",
      "0              Is salt unhealthy, Salt damages cells           0   \n",
      "1                     What are signs of intelligence           1   \n",
      "2  What are signs of intelligence, What are signs...           0   \n",
      "3                        How to gain weight quickly            0   \n",
      "4     What is arthritis, What is arthritis caused by           2   \n",
      "\n",
      "                                  Matching Sentences  \n",
      "0                                                     \n",
      "1  All hyperactive children have potential genius...  \n",
      "2                                                     \n",
      "3                                                     \n",
      "4  The cooked saturated fat are a problem, but th...  \n"
     ]
    }
   ],
   "source": [
    "def normalize_sentence(sentence):\n",
    "    # Need to remove any extra spaces/linebreaks\n",
    "    # The original df sentences sometimes keep their raw formatting (weird line breaks)\n",
    "    # This leads to a failure to match the embedding search results to my manually curated results\n",
    "    sentence = sentence.replace(\"\\n\", \" \")\n",
    "    return sentence.strip()\n",
    "\n",
    "def compute_evaluations(test_set, relevant_results):\n",
    "    evaluation_data = []\n",
    "\n",
    "    # My manually selected results (from keyword search)\n",
    "    # are grouped by a set of possible analogous queries.\n",
    "    # This loop iterates over each row that contains the same query group.\n",
    "    # After each row in the matching column has been iterated over, it iterates over the rows for the next query group.\n",
    "    for query_group in relevant_results['Query'].unique():\n",
    "        grouped_queries = query_group.split(',')\n",
    "        # Create a list of the normalised results from the relevant result dataframe (filtered on the query group)\n",
    "        relevant_set = [normalize_sentence(sentence) for sentence in relevant_results[relevant_results['Query'] == query_group]['Result']]\n",
    "        \n",
    "        total_hits = 0\n",
    "        matching_sentences = []\n",
    "\n",
    "        # Iterate through the queries in the query group, e.g. ('What is arthritis', 'What is arthritis caused by')\n",
    "        for query in grouped_queries:\n",
    "            query = query.strip()\n",
    "            # Find all the results from the test_set that pertain to the individual query (results from embedding search)\n",
    "            query_results = test_set[test_set['Query'] == query]['Result'].apply(normalize_sentence)\n",
    "\n",
    "            # Count the sentences that appear in both my manually collated relevant_results and the embedding search results\n",
    "            for sentence in query_results:\n",
    "                if sentence in relevant_set:\n",
    "                    total_hits += 1\n",
    "                    matching_sentences.append(sentence)\n",
    "\n",
    "        evaluation_data.append({\n",
    "            'Query Group': query_group,\n",
    "            'Total Hits': total_hits,\n",
    "            'Matching Sentences': ', '.join(matching_sentences)\n",
    "        })\n",
    "    \n",
    "    eval_df = pd.DataFrame(evaluation_data)\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "EVAL_DIR = Path.cwd() / \"evaluations\"\n",
    "EVAL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "relevant_results_path = QUERIES_DIR / \"relevant_query_results.csv\"\n",
    "relevant_results = pd.read_csv(relevant_results_path)\n",
    "\n",
    "test_set_path = TEST_SET_DIR / f\"test_set.csv\"\n",
    "test_set = pd.read_csv(test_set_path)\n",
    "\n",
    "eval_df = compute_evaluations(test_set, relevant_results)\n",
    "eval_path = EVAL_DIR / f\"evaluation.csv\"\n",
    "eval_df.to_csv(eval_path, index=False)\n",
    "\n",
    "print(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177e185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
